<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Multi-Petabyte Data Distribution in Industry & Science with CernVM File System</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white-fosdem-ish.css">
		<link rel="stylesheet" type="text/css" href="css/asciinema-player.css" />

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-auto-animate>
					<!--
						QR code generation command:
						qrencode -s 4 -o assets/img/this-talk-schedule.qr.png https://fosdem.org/2026/schedule/event/9YET9Y-cvmfs/
						qrencode -s 4 -o assets/img/this-talk-slides.qr.png https://autkin.net/talks/cvmfs-fosdem26-talk/slides/
					-->
					<img src="assets/img/fosdem-gear.svg">
					<div id="talk-title">
						Multi-Petabyte Data Distribution in Industry & Science with CernVM File System
					</div>
					<br>

					<div id="title-slide-footer">
						<ul style="list-style-type: none;">
							<li>Georgios Christodoulis, CERN</li>
							<li>Andriy Utkin</li>
						</ul>
						<center><small>2026</small></center>
						<div>
							Slides:
							<img src="assets/img/this-talk-slides.qr.png" style="vertical-align:middle">
						</div>
					</div>
				</section>

				<section data-auto-animate>
					<img src="assets/img/fosdem-gear.svg">

					<div id="talk-title">
						<h1 style="font-size: 300px">CVMFS</h1>
					</div>
					<br>
					<br>
					<br>
					<br>

					<div id="title-slide-footer">
						<ul style="list-style-type: none;">
							<li>Georgios Christodoulis, CERN</li>
							<li>Andriy Utkin</li>
						</ul>
						<center><small>2026</small></center>
						<div>
							Slides:
							<img src="assets/img/this-talk-slides.qr.png" style="vertical-align:middle">
						</div>
					</div>
				</section>

				<section data-markdown>
					<!--
						QR code generation command:
						qrencode -s 4 -o assets/img/bof-schedule.qr.png https://fosdem.org/2026/schedule/event/W93GZR-cvmfs-install-party/
					-->
					<textarea data-template>
					## CVMFS install party + adoption Q&A (BOF)

					[![](assets/img/bof-schedule.png)](https://fosdem.org/2026/schedule/event/W93GZR-cvmfs-install-party/)

					<div>
						Event:
						<img src="assets/img/bof-schedule.qr.png" style="vertical-align:middle">
					</div>

					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
					## What is CVMFS?

					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## Open Source at CERN

						stand @ K level 1 (group A)

						![map](assets/img/map-k1.png)
					</textarea>
				</section>


				<section data-markdown>
					<textarea data-template>
						### Software distribution in HEP

						![](assets/img/george/www.png) ![](assets/img/george/www_graphed.png)
						<aside class="notes">
							Something that people may not be very aware of is the amount of data that experiments in HEP are producing! The CERN Data Center stores more than 30 petabytes of data per year from the LHC experiments.
							These data along with the software used for their processing and analysis needs to be further distributed to laboratories all over the world.
						</aside>
					</textarea>
				</section>
        <section data-markdown>
					<textarea data-template>
						### CernVM Filesystem (CVMFS)
						![](assets/img/george/eeassi.png) ![](assets/img/george/software_stack.png)
						<aside class="notes">
              And here comes CVMFS.-----
              CVMFS is a read-only filesystem that for the end user provides an experience similar to an on-demand streaming service (but for scientific software).------
              As an example of access here I have used EESSI, the European Environment for Scientific Software Installations, whose goal is to create a uniform user experience for people who are using scientific software regardless of the system they are experimenting on, and they are using CVMFS as a part of their software stack. I would definitely recommend attending their presentation tomorrow. ------
              From the users point of view here, the interesting observation is that CVMFS provides a regular RO filesystem view of software that lives elsewhere, downloading only the essential parts upon access.
						</aside>
					</textarea>
				</section>
        <section data-markdown>
					<textarea data-template>
						### CernVM Filesystem (CVMFS)
						![](assets/img/george/fuse.png)
						<aside class="notes">
              But how does this work?
              CVMFS is a filesystem, that is living in userspace.
              Under the hood it is using FUSE, which allows for regular filesystem handling in userspace.
              FUSE comes with a kernel module and a  userspace library.
              When a user is accessing a file, vfs will redirect the call to the FUSE Kernel module, which will then get propagated to CVMFS that implements all the necessary RO syscalls.
              If the file is present in the local cache, it will be used directly, if not it will be first downloaded.
              To satisfy the the size restrictions of the client cache, CVMFS is using an LRU cleanup policy.
						</aside>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						## File distribution

						* Files are stored by content using hash (Merkle) trees
						* Allows for data verification and compression 
						* LRU cache on client side
						* Content addressable storage allows de-duplication (each hash is stored only once)

						![](assets/img/george/publishing.png)
						<aside class="notes">
              But how do files end up to the client?
              Files are becoming available to the client by a process called publishing.
              During publishing files are chunked and hashed into hash trees, specifically Merkle trees (same format is used on git) and compressed.
              Internally they are addressed by content not by their name or path which allows for de-duplication, which means that since files are stored by their hashed content, same hash is never saved twice resulting in significant reduction of used space.
              An order of magnitude of the impact of deduplication we can see later.
              After publishing the files are available in mirror servers, where can be fetched from the client via HTTP requests.
						</aside>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### CVMFS at CERN with numbers

            * ~33B files
            * ~2PB of data
						* 8k container images
						* ~290 repositories

						![](assets/img/george/cvmfs_files_at_CERN.png)
						<aside class="notes">
              CVMFS has been in development for quite long and it has been already widely adopted by the HEP community.
              You can see the evolution of files under /cvmfs from the main CERN repositories up until 2024 (we need to get our numbers up to date).
              CERN is hosting around 260 repositories, distributing more than 2PB of deduplicated content.
						</aside>
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						### Distribution of Container Images

            <strong>Unseekable</strong>  and **unindexed** file

						![](assets/img/george/cvmfs_files_at_CERN.png)
          <aside class="notes">
              CVMFS has been in development for quite long and it has been already widely adopted by the HEP community.
              You can see the evolution of files under /cvmfs from the main CERN repositories up until 2024 (we need to get our numbers up to date).
              CERN is hosting around 260 repositories, distributing more than 2PB of deduplicated content.
						</aside>
					</textarea>
				</section>

        <section data-markdown>
					<textarea data-template>
						### CernVM Filesystem (CVMFS)

            ```bash
            $ ls /cvmfs/software.eessi.io/
            host init README.eessi versions
            $ cat /cvmfs/software.eessi.io #JIT download
            EESSI - the European Environment for Scientific Software Installations

            Getting started
            --------------
            ```

						<aside class="notes">
              And here comes CVMFS.-----
              CVMFS is a read-only filesystem that for the end user provides an experience similar to an on-demand streaming service (but for scientific software).------
              As an example of access here I have used EESSI, the European Environment for Scientific Software Installations, whose goal is to create a uniform user experience for people who are using scientific software regardless of the system they are experimenting on, and they are using CVMFS as a part of their software stack. I would definitely recommend attending their presentation tomorrow. ------
              From the users point of view here, the interesting observation is that CVMFS provides a regular RO filesystem view of software that lives elsewhere, downloading only the essential parts upon access.
						</aside>
					</textarea>
				</section>


				<section data-markdown>
					<textarea data-template>
						## Lazy-pulling<br>container images

					</textarea>
				</section>

				<section>
					<aside class="notes">
						Here we use the lazy pulling.

						Here we use a popular image slightly bigger than 1 Gigabyte.

						This image is developed and distributed by and for scientists, and is somewhat popular.

						It takes 25 seconds to fetch whatever is strictly necessary and execute the python process which
						loads the target module.

						Afterwards, the CVMFS cache is 600 Megabytes.
						So we skipped downloading quite a lot.
					</aside>

					Lazy pulling in action

					<iframe src='demo-lazy-pulling-1.html'
						width="1920px" height="1080px"
					>
					</iframe>
				</section>

				<section>
					<aside class="notes">
						You can, of course, use the same container image the usual way: issue a normal podman run
						command and let it download the image.

						This demo was recorded on a very fast connection, by the way, the uplink is upward of a gigabit.

						(Wait for the animation to end with the "no space left on device")

						Unless it doesn't fit on your disk, in which case you'd have to go on a cleanup diversion or
						resize your machine.
					</aside>

					Normal image pulling

					<iframe src='demo-lazy-pulling-2.html'
						width="1920px" height="1080px"
					>
					</iframe>
				</section>

				<section data-markdown>
					<textarea data-template>
					## References

					[doc](https://cvmfs.readthedocs.io/en/stable/cpt-containers.html),
					KubeCon25
					[slides](https://static.sched.com/hosted_files/kccnceu2025/f3/ImageSnapshottersForEfficientContainerExecutionInParticlePhysics.pdf),
					[video](https://www.youtube.com/watch?v=Dc6S4vU9GiM)
					</textarea>
				</section>

				<section>

					<h2>cvmfs-posix-tools</h2>

					<aside class="notes">

						These are new tools, modelled after popular file utilities, which offer the convenience of
						direct effect on CVMFS repo.
						<br>

						Can be seen as easier to use tools.
						<br>

						They provide speedups in some modes.
						<br>

						They can help avoid running out of disk space in scratch area.

					</aside>

					<!-- Deemphasize common prefix, emphasize second half of command names -->
					<ul style="columns: 2; list-style-type: none;">
						<li>cvmfs_<strong>chgrp</strong></li>
						<li>cvmfs_<strong>chmod</strong></li>
						<li>cvmfs_<strong>chown</strong></li>
						<li>cvmfs_<strong>insert</strong></li>
						<li>cvmfs_<strong>ln</strong></li>
						<li>cvmfs_<strong>mkdir</strong></li>
						<li>cvmfs_<strong>rm</strong></li>
						<li>cvmfs_<strong>rmdir</strong></li>
						<li>cvmfs_<strong>rsync</strong></li>
						<li>cvmfs_<strong>setfacl</strong></li>
						<li>cvmfs_<strong>touch</strong></li>
					</ul>

				</section>

				<section data-markdown>
					<textarea data-template>
						## Changing contents,<br>the usual way

						* `cvmfs_server transaction`
						* make changes
						* `cvmfs_server publish`

						<aside class="notes">
							To explain why they are needed, let's briefly see how changing contents <strong>currently</strong> works from the user perspective.<br>
							The first method you learn from the documentation to change the contents of your repo is this:

							* `cvmfs_server transaction`
							* make changes
							* `cvmfs_server publish`

						</aside>
					</textarea>
				</section>

				<!--
				<section>
					<aside class="notes">
					</aside>

					How publishing looks

					<iframe src='publish-demo.html'
						width="1920px" height="1080px"
					>
					</iframe>
				</section>
				-->
				<section>
					<aside class="notes">
						<p>
						This is publish method adding a 4 Gigabyte file into an S3-backed CVMFS repository.
						</p><p>
						`transaction` command changes the overlay mount from read-only to read-write.
						</p><p>
						The file first ends up in the scratch dir.
						</p><p>
						And after the `publish` command, the file is in the actual CVMFS mount.
						</p>
					</aside>

					Publish command in action

					<iframe src='demo-publish-bigmodel.html'
							width="1920px" height="1080px"
					>
					</iframe>
				</section>

				<section data-markdown>
					<textarea data-template>
						<aside class="notes">
						<p>
							Importantly, in this way, two expensive steps must occur to accomplish the task of publishing a new file.
						</p><p>
							First, the new file is copied into local scratch area.
						</p><p>
							SKIPPABLE BEGIN
							This is dealt by overlayfs, it provides a nice illusion of editing the filesystem directly, and it enables the next step, the publish command, to instantly see what are the changed files to upload.
							SKIPPABLE END
						</p><p>
							Second, the copy in the scratch area is transferred to the primary storage of CVMFS repo.
						</p><p>
							It can be a local directory if we are working directly on the main server, called Stratum 0.
						</p><p>
							It can be S3 location.
						</p><p>
							And it can be a remote CVMFS Gateway service.
						</p>
						</aside>

						<table width="100%">
							<tr width="100%">
								<td width="75%">
									<code style="white-space: nowrap;">cvmfs_server transaction</code>
								</td>
								<td width="25%">
									change overlayfs mode
								</td>
							</tr>
							<tr width="100%">
								<td width="75%">
									<code style="white-space: nowrap;">cp bigmodel.gguf /cvmfs/my.repo</code>
								</td>
								<td width="25%">
									copy to scratch area
								</td>
							</tr>
							<tr width="100%">
								<td width="60%">
									<code style="white-space: nowrap;">cvmfs_server publish</code>
								</td>
								<td width="25%">
									copy to primary storage
								</td>
							</tr>
						</table>

					</textarea>
				</section>

				<section>
					<aside class="notes">
					    You can also choose whether to store new files in Content Addressable Storage fashion, or
						represent the tree hierarchy and original filenames in S3 locations.
					</aside>

					cvmfs_rsync in action

					<iframe src='demo-cvmfs_rsync-external-faster-plus-s3-listing.html'
						width="1920px" height="1080px"
					>
					</iframe>
				</section>


				<section data-markdown>
					<textarea data-template>
						## cvmfs-posix-tools:<br>under the hood

						- uploads all the new files to S3 directly
						- pushes metadata changes to CVMFS Gateway host
						- waits for new CVMFS revision to become visible

						<aside class="notes">
							Note: this doesn't mean you need AWS, many protocol implementations work - Ceph, Garage,
							MinIO, Azure has some quirk but also works.
						</aside>

					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					## References

					Container Images and CVMFS:
					<br>
					[doc](https://cvmfs.readthedocs.io/en/stable/cpt-containers.html),
					KubeCon25
					[slides](https://static.sched.com/hosted_files/kccnceu2025/f3/ImageSnapshottersForEfficientContainerExecutionInParticlePhysics.pdf),
					[video](https://www.youtube.com/watch?v=Dc6S4vU9GiM)

					cvmfs-posix-tools:
					[pull request](https://github.com/cvmfs/cvmfs/pull/3964)
					</textarea>
				</section>

				<section>
						<h2>Q & A</h2>
						<center>
						<table width=100%>
							<tr>
								<td>
									Georgios Christodoulis, CERN<br>
									<span style="font-style: italic">georgios.christodoulis@cern.ch</span>
								</td>
								<td>
									Andriy Utkin<br>
									<span style="font-style: italic">hello@autkin.net</span>
								</td>
							</tr>
							<tr>
								<td colspan="2">
									<img src="assets/img/bof-schedule.png" width="70%">

								</td>
							</tr>
							<tr>
								<td>
									Open&nbsp;Source&nbsp;at&nbsp;CERN
								</td>
								<td>
									<img src="assets/img/map-k1.png" width="70%" style="vertical-align:middle">
								</td>
							</tr>
						</table>
						</center>
				</section>

				<section data-markdown>
					<!--
						QR code generation command:
						qrencode -s 4 -o assets/img/bof-schedule.qr.png https://fosdem.org/2026/schedule/event/W93GZR-cvmfs-install-party/
					-->
					<textarea data-template>
					## CVMFS install party + adoption Q&A (BOF)

					[![](assets/img/bof-schedule.png)](https://fosdem.org/2026/schedule/event/W93GZR-cvmfs-install-party/)

					<div>
						Event:
						<img src="assets/img/bof-schedule.qr.png" style="vertical-align:middle">
					</div>

					</textarea>
				</section>


				<section data-markdown>
					<textarea data-template>
						## Open Source at CERN

						stand @ K level 1 (group A)

						![map](assets/img/map-k1.png)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					## Backup slides

					TODO

					</textarea>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="js/asciinema-player.min.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
			// https://github.com/mickaelbaron/revealasciinema/blob/master/workaround.html
			// https://discourse.asciinema.org/t/asciinema-reveal-js-display-issue/618/3
			/*
			{
				const asciinemas = document.getElementsByClassName("asciinema");
				for (let i = 0; i < asciinemas.length; i++) {
					const img = document.createElement("img");
					img.setAttribute("src", "assets/img/asciinema-player-icon.svg");
					asciinemas[i].appendChild(img);
				}
			}
			*/
			function checkForCasts(event) {
				const asciinema =
					event.currentSlide.getElementsByClassName("asciinema")[0];
				if (asciinema) {
					const filename = asciinema.getAttribute("file");
					if (asciinema && !asciinema.playerObject) {
						// Remove image
						const firstelementChild = asciinema.firstElementChild;
						if (firstelementChild) {
							asciinema.firstElementChild.remove();
						}

						asciinema.playerObject = AsciinemaPlayer.create(
							filename,
							asciinema,
							{ fit: "none" }
						);
					}
				}
			}
			Reveal.on("ready", checkForCasts);
			Reveal.on("slidechanged", checkForCasts);
		</script>
	</body>
</html>
